{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Pe7_VjqBdVz"
      },
      "outputs": [],
      "source": [
        "# Unzip the uploaded dataset\n",
        "!unzip -o university_chatbot_dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Display a sample\n",
        "for intent in data[\"intents\"]:\n",
        "    print(intent)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "P1tGfZEhGBs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "0CGbtb2-Gs8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Optional: improves lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')  # Sometimes helpful\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "hpySI06yHCbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load intents.json\n",
        "with open('intents.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!', '.', ',']\n",
        "\n",
        "# Fix for your dataset (text and intent instead of patterns and tag)\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['text']:\n",
        "        word_list = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list, intent['intent']))\n",
        "        if intent['intent'] not in classes:\n",
        "            classes.append(intent['intent'])\n",
        "\n",
        "# Lemmatize and sort\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "# Save words and classes\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
        "\n",
        "# Create training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "    bag = []\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in document[0]]\n",
        "\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# Shuffle and convert to numpy array\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "train_x = np.array(list(training[:, 0]))\n",
        "train_y = np.array(list(training[:, 1]))\n",
        "\n",
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile and train\n",
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\n",
        "\n",
        "# Save model\n",
        "model.save('model.h5')\n",
        "print(\"‚úÖ Model trained and saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uIhOSb63FjgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load all files\n",
        "model = load_model('model.h5')\n",
        "intents = json.loads(open('intents.json').read())\n",
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "\n",
        "# Clean up user input\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Convert input into bag of words\n",
        "def bow(sentence, words):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag)\n",
        "\n",
        "# Predict the intent\n",
        "def predict_class(sentence):\n",
        "    bow_vector = bow(sentence, words)\n",
        "    res = model.predict(np.array([bow_vector]))[0]\n",
        "    threshold = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > threshold]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "# Get response from intents.json\n",
        "def get_response(intents_list, intents_json):\n",
        "    if len(intents_list) == 0:\n",
        "        return \"I'm not sure how to respond to that.\"\n",
        "    tag = intents_list[0]['intent']\n",
        "    for i in intents_json['intents']:\n",
        "        if i['intent'] == tag:\n",
        "            return random.choice(i['responses'])\n",
        "\n",
        "# Chat with the bot\n",
        "def chatbot_response(text):\n",
        "    intents_list = predict_class(text)\n",
        "    response = get_response(intents_list, intents)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "3FFM1CaAIEln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatbot_response(\"hello\"))\n",
        "print(chatbot_response(\"can you help me?\"))\n"
      ],
      "metadata": {
        "id": "QH0MltyXIUnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nYou: can I get info about admission?\")\n",
        "print(\"Bot:\", chatbot_response(\"can I get info about admission?\"))"
      ],
      "metadata": {
        "id": "BTj846qpInim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nYou: where  i can get the admit card\")\n",
        "print(\"Bot:\", chatbot_response(\"where  i can get the admit card\"))"
      ],
      "metadata": {
        "id": "JwU5JbQyIrxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nYou: what is the process of admission?\")\n",
        "print(\"Bot:\", chatbot_response(\"what is the process of admission?\"))"
      ],
      "metadata": {
        "id": "HTM_pLkOJFNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load intents.json\n",
        "with open(\"intents.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Search for intents related to \"apply\" or \"admission\"\n",
        "keywords = [\"apply\", \"admission\", \"procedure\", \"how to apply\"]\n",
        "found = False\n",
        "\n",
        "print(\"üîç Searching for relevant intents...\\n\")\n",
        "for intent in data[\"intents\"]:\n",
        "    all_texts = intent.get(\"text\", [])\n",
        "    for phrase in all_texts:\n",
        "        if any(keyword.lower() in phrase.lower() for keyword in keywords):\n",
        "            print(f\"‚úÖ Found in intent: {intent['intent']}\")\n",
        "            print(\"Examples:\", all_texts)\n",
        "            print(\"Responses:\", intent['responses'])\n",
        "            found = True\n",
        "            print(\"-\" * 60)\n",
        "            break\n",
        "\n",
        "if not found:\n",
        "    print(\"‚ùå No intent found for admission/apply-related questions.\")\n"
      ],
      "metadata": {
        "id": "cpukEmImJbqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatbot_response(\"how do I apply?\"))\n",
        "print(chatbot_response(\"hello\"))\n",
        "print(chatbot_response(\"where is the campus?\"))\n"
      ],
      "metadata": {
        "id": "Eu6kNYQ6MJDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep -E 'pandas|tensorflow|matplotlib|seaborn|scikit-learn'\n"
      ],
      "metadata": {
        "id": "D-7uoJipR5LS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}